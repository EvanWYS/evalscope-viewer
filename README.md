# EvalScope Viewer

A lightweight, static visualization platform for LLM evaluation results. Designed for evalscope with extensibility for other evaluation frameworks.

## ğŸ¯ Features

- **Framework-Agnostic Architecture**: Currently supports evalscope with easy extensibility for other frameworks (lm-harness, OpenCompass, etc.)
- **Static-First Design**: No backend required - pure static files deployed anywhere
- **Comprehensive Visualization**:
  - Run listings with metadata
  - Detailed benchmark results with category breakdowns
  - Sample-level prediction inspection
- **Modern UI**: Built with Next.js 14, TypeScript, and Tailwind CSS
- **Extensible**: Adapter pattern for easy integration of new evaluation frameworks

## ğŸ“ Project Structure

```
evalscope-viewer/
â”œâ”€â”€ tools/etl/              # ETL pipeline (Python)
â”‚   â”œâ”€â”€ core/              # Framework-agnostic core
â”‚   â”œâ”€â”€ adapters/          # Framework-specific adapters
â”‚   â”‚   â”œâ”€â”€ base.py       # Abstract adapter interface
â”‚   â”‚   â””â”€â”€ evalscope/    # EvalScope adapter
â”‚   â””â”€â”€ build_static_data.py
â”‚
â”œâ”€â”€ web/                   # Frontend (Next.js)
â”‚   â”œâ”€â”€ app/              # Pages
â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”œâ”€â”€ lib/              # Utilities & types
â”‚   â””â”€â”€ public/data/      # Generated static JSON
â”‚
â”œâ”€â”€ outputs/              # EvalScope raw output (not in git)
â””â”€â”€ dcos/                 # Design documents
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- npm or yarn

### Step 1: Install Dependencies

```bash
# Install ETL dependencies
pip install -r tools/etl/requirements.txt

# Install frontend dependencies
cd web
npm install
```

### Step 2: Generate Data

Place your evalscope output in the `outputs/` directory, then run:

```bash
python tools/etl/build_static_data.py \
  --framework evalscope \
  --raw-dir ./outputs \
  --out-dir ./web/public/data
```

### Step 3: Start Development Server

```bash
cd web
npm run dev
```

Visit http://localhost:3000 to view your evaluation results.

### Step 4: Build for Production

```bash
cd web
npm run build
```

The static site will be generated in `web/out/`. Deploy this directory to any static hosting service.

## ğŸ“Š Data Flow

```
EvalScope Evaluation
        â†“
   Raw Output Files
   (configs, reports, predictions)
        â†“
   ETL Pipeline
   (Extract â†’ Transform â†’ Load)
        â†“
   Standard JSON Protocol
   (meta.json, eval_summary.json, samples/*.jsonl)
        â†“
   Next.js Frontend
   (Static visualization)
```

## ğŸ”§ ETL Pipeline

The ETL pipeline converts framework-specific output to a standard JSON protocol:

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Adapter Layer (Framework-Specific)  â”‚
â”‚  - evalscope adapter            â”‚
â”‚  - Future: lm-harness adapter   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Core Layer (Framework-Agnostic)     â”‚
â”‚  - Standard data models         â”‚
â”‚  - JSON schema definitions      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output (Static JSON)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Usage

```bash
# Basic usage
python tools/etl/build_static_data.py \
  --framework evalscope \
  --raw-dir ./outputs \
  --out-dir ./web/public/data

# Custom sample limit
python tools/etl/build_static_data.py \
  --framework evalscope \
  --raw-dir ./outputs \
  --out-dir ./web/public/data \
  --sample-limit 200

# Process specific runs
python tools/etl/build_static_data.py \
  --framework evalscope \
  --raw-dir ./outputs \
  --out-dir ./web/public/data \
  --run-pattern "20251124_*"
```

## ğŸ“ Standard JSON Protocol

The ETL pipeline generates these files:

### index.json
```json
{
  "runs": [
    {
      "run_id": "run_20251124_143025_a1b2c3d4",
      "framework": "evalscope",
      "model": { "name": "Qwen/Qwen2.5-7B", "type": "openai_api" },
      "datasets": ["mmlu", "gsm8k"],
      "overall_score": 0.68,
      "status": "completed"
    }
  ]
}
```

### runs/<run_id>/meta.json
Run metadata including model config, timestamps, and environment.

### runs/<run_id>/eval_summary.json
Aggregated evaluation results with metrics and category breakdowns.

### runs/<run_id>/samples/<dataset>_head.jsonl
Sample-level predictions (JSONL format).

See `dcos/ETL Data Protocol.md` for complete specification.

## ğŸ”Œ Adding New Frameworks

To add support for a new evaluation framework:

1. **Create adapter directory**:
   ```bash
   mkdir -p tools/etl/adapters/myframework
   ```

2. **Implement adapter**:
   ```python
   # tools/etl/adapters/myframework/adapter.py
   from ..base import BaseAdapter
   from ...core.models import StandardRunMeta, StandardBenchmarkResult

   class MyFrameworkAdapter(BaseAdapter):
       def extract_meta(self) -> StandardRunMeta:
           # Parse framework-specific config
           ...

       def extract_results(self) -> List[StandardBenchmarkResult]:
           # Parse framework-specific results
           ...
   ```

3. **Register adapter**:
   ```python
   # tools/etl/adapters/__init__.py
   from .myframework import MyFrameworkAdapter

   ADAPTER_REGISTRY = {
       "evalscope": EvalScopeAdapter,
       "myframework": MyFrameworkAdapter,
   }
   ```

4. **Use it**:
   ```bash
   python tools/etl/build_static_data.py --framework myframework --raw-dir ... --out-dir ...
   ```

## ğŸ¨ Frontend

Built with modern web technologies:

- **Framework**: Next.js 14 (App Router)
- **Language**: TypeScript
- **Styling**: Tailwind CSS
- **Charts**: ECharts (for future enhancements)
- **Deployment**: Static export (no server required)

### Key Pages

- `/` - List all evaluation runs
- `/runs/[runId]` - Run details with benchmark results
- `/runs/[runId]/samples` - Sample predictions viewer

## ğŸ“– Documentation

- [Evaluation Visualization System Design](dcos/Evaluation%20Visualization%20System%20Design.md)
- [ETL Data Protocol](dcos/ETL%20Data%20Protocol.md)
- [ETL README](tools/etl/README.md)

## ğŸ›£ï¸ Roadmap

### Phase 1: MVP (Current)
- âœ… ETL pipeline with evalscope adapter
- âœ… Frontend with runs list, details, and samples
- âœ… Standard JSON protocol

### Phase 2: Enhanced Visualization
- â³ ECharts integration for metrics visualization
- â³ Category comparison charts
- â³ Multi-run comparison view

### Phase 3: Multi-Framework Support
- â³ lm-evaluation-harness adapter
- â³ OpenCompass adapter
- â³ Plugin system for custom adapters

### Phase 4: Advanced Features
- â³ Error analysis dashboard
- â³ Filtering and search
- â³ Export reports

## ğŸ¤ Contributing

Contributions are welcome! Areas of interest:

- New framework adapters
- Visualization components
- Documentation improvements
- Bug fixes and optimizations

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- Built for evalscope evaluation framework
- Designed for extensibility to support the broader LLM evaluation ecosystem
